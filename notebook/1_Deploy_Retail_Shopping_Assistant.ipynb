{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Blueprint: Retail Shopping Assistant\n",
    "\n",
    " This notebook will deploy the **Retail Shopping Assistant** AI Blueprint. You will install the necessary prerequisites, spin up the NVIDIA NIMâ„¢ microservices (either locally or using cloud endpoints), and deploy the multi-agent retail shopping system. Once deployed, you will have a fully functional reference UI as well as sample code which you can use to build intelligent retail shopping experiences with features like product search, cart management, visual search, and conversational AI interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram\n",
    "![shopping-assistant-diagram.jpg](attachment:shopping-assistant-diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    ">[Prerequisites](#Prerequisites)  \n",
    ">[Environment Setup](#Environment-Setup)  \n",
    ">[Configuration Options](#Configuration-Options)  \n",
    ">[Deployment Options](#Deployment-Options)  \n",
    ">[Local NIM Deployment](#Local-NIM-Deployment)  \n",
    ">[Cloud NIM Deployment](#Cloud-NIM-Deployment)  \n",
    ">[Application Deployment](#Application-Deployment)  \n",
    ">[Validate Deployment](#Validate-Deployment)  \n",
    ">[API Reference](#API-Reference)  \n",
    ">[Next Steps](#Next-Steps)  \n",
    ">[Shutting Down Blueprint](#Stopping-Services-and-Cleaning-Up)  \n",
    ">[Appendix](#Appendix)  \n",
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone repository and install software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Clone** Retail Shopping Assistant Git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/NVIDIA-AI-Blueprints/retail-shopping-assistant.git #Using HTTP\n",
    "!git clone git@github.com:NVIDIA-AI-Blueprints/retail-shopping-assistant.git #Using SSH\n",
    "!cd retail-shopping-assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install **[Docker](https://docs.docker.com/engine/install/ubuntu/)**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> Ensure the Docker Compose plugin version is 2.29.1 or higher.  Run `docker compose version` to confirm. Refer to Install the Compose plugin Docker documentation for more information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install **[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit)** to configure Docker for GPU-accelerated containers, for example Milvus, NVIDIA NIM.\n",
    " If you are using a system deployed with Brev you can skip this step since Brev systems come with NVIDIA Container Toolkit preinstalled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Requirements\n",
    "\n",
    "#### For Local NIM Deployment (Recommended)\n",
    "- **GPUs**: 4x H100 GPUs (or equivalent)\n",
    "- **RAM**: 128GB+ system memory\n",
    "- **Storage**: 100GB+ available disk space\n",
    "- **CPU**: 16+ cores\n",
    "\n",
    "#### For Cloud NIM Deployment\n",
    "- **RAM**: 32GB+ system memory\n",
    "- **Storage**: 50GB+ available disk space\n",
    "- **CPU**: 8+ cores\n",
    "- **Network**: Stable internet connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Use oauthtoken as the username and your API key as the password. The $oauthtoken username is a special name that indicates that you will authenticate with an API key and not a user name and password.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### Let's set the NVIDIA API Catalog key.\n",
    "\n",
    "This NVIDIA API Catalog key will be used to access cloud hosted models in API Catalog.\n",
    "\n",
    "You can use different model API endpoints with the same API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select a model, such as llama3-8b-instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select an **Input** option. The following example is of a model that offers a Docker option. Not all of the models offer this option, but all include a \"Get API Key\" link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Click **Get API Key**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Select **\"Generate Key\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Copy your key** and store it in a secure place. Do not share it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> The key begins with the letters nvapi-.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Input your key to the local environment using the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's log into the NVIDIA Container Registry.\n",
    " \n",
    "The NVIDIA NGC API Key is a mandatory key that is required to use this blueprint. This is needed to log into the NVIDIA container registry, nvcr.io, and to pull secure container images used in this NVIDIA NIM Blueprint.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) in the NVIDIA NGC User Guide for more information.\n",
    "\n",
    "Authenticate with the NVIDIA Container Registry with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login nvcr.io --username '$oauthtoken' --password $NVIDIA_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will set up the environment variables and create necessary directories. Here, each enfironment variable controls a different model container within the model. By default we set all of these to use our NVIDIA_API_KEY, but you can use different keys and different endpoints for your embedding model, LLM, and guardrails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"NGC_API_KEY\"] = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    "os.environ[\"LLM_API_KEY\"] = os.environ[\"NGC_API_KEY\"]\n",
    "os.environ[\"EMBED_API_KEY\"] = os.environ[\"NGC_API_KEY\"]\n",
    "os.environ[\"RAIL_API_KEY\"] = os.environ[\"NGC_API_KEY\"]\n",
    "\n",
    "# Create NIM cache directory\n",
    "local_nim_cache = os.path.expanduser(\"~/.cache/nim\")\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = local_nim_cache\n",
    "os.makedirs(local_nim_cache, exist_ok=True)\n",
    "os.system(f\"chmod a+w {local_nim_cache}\")\n",
    "\n",
    "print(f\"Environment variables set\")\n",
    "print(f\"NIM cache directory: {local_nim_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Retail Shopping Assistant supports a flexible configuration override system that allows you to switch between different deployment scenarios without modifying the base configuration files.\n",
    "\n",
    "### How Configuration Override Works\n",
    "\n",
    "1. **Base Configuration**: The application loads the base `config.yaml` file\n",
    "2. **Override Detection**: If the `CONFIG_OVERRIDE` environment variable is set, the system looks for an override file\n",
    "3. **Merge Process**: The override file values are merged into the base configuration, with override values taking precedence\n",
    "\n",
    "### Available Configuration Options\n",
    "\n",
    "| Configuration | Environment Variable | Description |\n",
    "|---------------|---------------------|-------------|\n",
    "| Local NIMs | `CONFIG_OVERRIDE=config-local.yaml` | Use locally deployed NIMs |\n",
    "| Cloud NIMs | `CONFIG_OVERRIDE=config-build.yaml` | Use NVIDIA API Catalog hosted endpoints |\n",
    "| Custom | `CONFIG_OVERRIDE=config-custom.yaml` | Use your own custom configuration |\n",
    "| Default | (not set) | Use base configuration only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Retail Shopping Assistant supports two deployment options:\n",
    "\n",
    "### Option 1: Local NIM Deployment (Recommended)\n",
    "- **Best for**: Development, testing, production with GPU resources\n",
    "- **Requirements**: 4x H100 GPUs, 128GB+ RAM\n",
    "- **Pros**: Maximum performance, complete privacy, no ongoing cloud costs\n",
    "- **Cons**: Higher hardware investment, more complex setup\n",
    "\n",
    "### Option 2: Cloud NIM Deployment\n",
    "- **Best for**: Development, testing, production without local GPUs\n",
    "- **Requirements**: Stable internet connection\n",
    "- **Pros**: No local GPU requirements, faster setup, pay-per-use\n",
    "- **Cons**: Ongoing cloud costs, network latency, API rate limits\n",
    "\n",
    "Choose your deployment option below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local NIM Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Configuration for Local NIMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration override for local NIM deployment\n",
    "os.environ[\"CONFIG_OVERRIDE\"] = \"config-local.yaml\"\n",
    "print(f\"Configuration override set to: {os.environ['CONFIG_OVERRIDE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Verify GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA drivers and GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy Local NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start the following NVIDIA NIM microservices:\n",
    "- **Llama 3.1 70B Instruct**: Large language model for conversation\n",
    "- **NV-CLIP**: Visual understanding model for image search\n",
    "- **NV-EmbedQA E5-v5**: Embedding model for semantic search\n",
    "- **Llama 3.1 Nemoguard Content Safety**: Content moderation\n",
    "- **Llama 3.1 Nemoguard Topic Control**: Topic control and safety\n",
    "\n",
    "Note, we are going to hide the outputs in order to not flood the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start local NIMs\n",
    "!cd .. && docker compose -f docker-compose-nim-local.yaml up -d > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> This may take up to **15 minutes** to complete as it downloads and initializes the NIM models. Once this block terminates the below block will monitor our progress launching our NIMs. When you run it and it says that the NIMs are available. You are ready to move on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor NIM startup progress\n",
    "!cd .. && docker compose -f docker-compose-nim-local.yaml logs --tail 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify NIM Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NIM container status\n",
    "!cd .. && docker compose -f docker-compose-nim-local.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command should produce similar output in the following format:\n",
    "\n",
    "```\n",
    "CONTAINER ID   NAMES                                   STATUS\n",
    "1dd42caad60e   retail-shopping-assistant-llama-1      Up 10 minutes\n",
    "766acb5fb57c   retail-shopping-assistant-nvclip-1     Up 10 minutes\n",
    "4c4d1136cd7a   retail-shopping-assistant-embedqa-1    Up 10 minutes\n",
    "ff2f71eb9d75   retail-shopping-assistant-content-1    Up 10 minutes\n",
    "fd70635efcac   retail-shopping-assistant-topic_control-1 Up 10 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud NIM Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Configuration for Cloud NIMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration override for cloud NIM deployment\n",
    "os.environ[\"CONFIG_OVERRIDE\"] = \"config-build.yaml\"\n",
    "print(f\"Configuration override set to: {os.environ['CONFIG_OVERRIDE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have local GPU resources, you can use cloud-hosted NIM endpoints. The application will automatically use the NVIDIA API Catalog hosted endpoints when local NIMs are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> When using cloud NIM deployment, you don't need to start the local NIM containers. The application will use the NVIDIA API Catalog hosted endpoints.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker compose scripts are provided which spin up the microservices on a single node. This docker-compose yaml file will start the retail shopping assistant services as well as dependent microservices. We will once-again hide the outputs from this as to not flood the notebook with print statements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> Refer to the docker-compose.yaml for complete details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and start application services\n",
    "!cd .. && docker compose -f docker-compose.yaml up -d --build > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Again, this can take up to 10 minutes, so sit tight. Once it is up we will check the logs in the following code blocks once it completes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> If you would like to monitor progress, refer to https://docs.docker.com/reference/cli/docker/compose/logs/.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor application startup\n",
    "!cd .. && docker compose -f docker-compose.yaml logs --tail 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the deployment of the blueprint, execute the following command to ensure the containers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && docker compose -f docker-compose.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command should produce similar output in the following format:\n",
    "\n",
    "```\n",
    "CONTAINER ID   NAMES                       STATUS\n",
    "e6e1f6ebec3c   shopping-frontend           Up 3 hours\n",
    "b6a1853c4e81   chain-server                Up 3 hours\n",
    "91487a937be1   catalog-retriever           Up 3 hours\n",
    "0112183489fe   memory-retriever            Up 3 hours\n",
    "9970bb569dbd   rails                       Up 3 hours\n",
    "4ea1a3267a17   milvus-standalone           Up 3 hours\n",
    "c988dcdd67c3   milvus-minio                Up 3 hours (healthy)\n",
    "3dc1c2262903   milvus-etcd                 Up 3 hours (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The NIM microservices are not listed since they are either running locally (if using local deployment) or using hosted endpoints (if using cloud deployment).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blueprint includes a reference UI and an AI assistant (developed using the LangGraph framework) that leverages sub-agents to handle retail shopping queries. Let's make sure the API endpoints and UI are up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test our server health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain-server health\n",
    "!curl -s http://localhost:8009/health | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retriever health\n",
    "!curl -s http://localhost:8010/health | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the memory server health\n",
    "!curl -s http://localhost:8011/health | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access the Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your browser and navigate to: `http://localhost:3000`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> If you are using an environment deployed with Brev, make sure to expose port 3000 on your Brev console and use the HTTP URL created.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed API references, please refer to the following locations in the Blueprint repository:\n",
    "\n",
    "- **Chain Server API**: `./docs/API.md`\n",
    "- **User Guide**: `./docs/USER_GUIDE.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your Retail Shopping Assistant is deployed, you can:\n",
    "\n",
    "1. **Explore the UI**: Test the conversational interface, product search, and cart management features\n",
    "2. **Customize the Application**: Modify the configuration files to adapt to your specific retail domain\n",
    "3. **Add Your Data**: Replace the sample data with your own product catalog and user data\n",
    "4. **Scale the Deployment**: Consider production deployment options for high availability\n",
    "5. **Monitor Performance**: Use the built-in monitoring and analytics features\n",
    "\n",
    "For detailed customization and development guides, see:\n",
    "- [User Guide](docs/USER_GUIDE.md)\n",
    "- [API Documentation](docs/API.md)\n",
    "- [Deployment Guide](docs/DEPLOYMENT.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Services and Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shut down the microservices, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop application services\n",
    "!cd .. && docker compose -f docker-compose.yaml down > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop local NIMs (if using local deployment)\n",
    "!cd .. && docker compose -f docker-compose-nim-local.yaml down > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching Between Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To switch between different configurations without redeploying:\n",
    "\n",
    "```bash\n",
    "# Switch to local NIMs\n",
    "export CONFIG_OVERRIDE=config-local.yaml\n",
    "docker compose -f docker-compose.yaml restart\n",
    "\n",
    "# Switch to cloud NIMs\n",
    "export CONFIG_OVERRIDE=config-build.yaml\n",
    "docker compose -f docker-compose.yaml restart\n",
    "\n",
    "# Use base configuration (no override)\n",
    "unset CONFIG_OVERRIDE\n",
    "docker compose -f docker-compose.yaml restart\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Configuration Overrides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own override files for custom configurations:\n",
    "\n",
    "1. **Create the override file** in the same directory as the base config:\n",
    "   ```bash\n",
    "   # For chain server\n",
    "   cp chain_server/app/config.yaml chain_server/app/config-custom.yaml\n",
    "   \n",
    "   # For catalog retriever\n",
    "   cp catalog_retriever/src/config.yaml catalog_retriever/src/config-custom.yaml\n",
    "   ```\n",
    "\n",
    "2. **Modify the override file** with your custom values:\n",
    "   ```yaml\n",
    "   # Example: Custom LLM endpoint\n",
    "   llm_port: \"https://your-custom-endpoint.com/v1\"\n",
    "   llm_name: \"your-custom-model\"\n",
    "   \n",
    "   # Example: Custom embedding endpoint\n",
    "   text_embed_port: \"https://your-embedding-service.com/v1\"\n",
    "   text_model_name: \"your-embedding-model\"\n",
    "   ```\n",
    "\n",
    "3. **Use the custom override**:\n",
    "   ```bash\n",
    "   export CONFIG_OVERRIDE=config-custom.yaml\n",
    "   docker compose -f docker-compose.yaml up -d --build\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Memory Issues\n",
    "If you encounter GPU memory issues with local NIM deployment:\n",
    "\n",
    "```bash\n",
    "# Check GPU memory usage\n",
    "nvidia-smi\n",
    "\n",
    "# Adjust GPU allocation in docker-compose-nim-local.yaml\n",
    "# Modify the device_ids for each service\n",
    "```\n",
    "\n",
    "#### Port Conflicts\n",
    "If ports are already in use:\n",
    "\n",
    "```bash\n",
    "# Check what's using the ports\n",
    "sudo netstat -tulpn | grep :8000\n",
    "sudo netstat -tulpn | grep :3000\n",
    "\n",
    "# Stop conflicting services or modify ports in docker-compose.yaml\n",
    "```\n",
    "\n",
    "#### NIM Startup Issues\n",
    "If NIMs fail to start:\n",
    "\n",
    "```bash\n",
    "# Check NIM logs\n",
    "docker compose -f docker-compose-nim-local.yaml logs -f\n",
    "\n",
    "# Verify NGC API key\n",
    "echo $NGC_API_KEY\n",
    "\n",
    "# Check disk space\n",
    "df -h\n",
    "```\n",
    "\n",
    "#### Configuration Override Issues\n",
    "If configuration overrides are not working:\n",
    "\n",
    "```bash\n",
    "# Check if override file exists\n",
    "ls -la chain_server/app/config-*.yaml\n",
    "ls -la catalog_retriever/src/config-*.yaml\n",
    "\n",
    "# Verify environment variable is set\n",
    "echo $CONFIG_OVERRIDE\n",
    "\n",
    "# Check application logs for config loading messages\n",
    "docker compose -f docker-compose.yaml logs chain-server | grep -i config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Allocation\n",
    "By default, GPU IDs are allocated as follows:\n",
    "- **GPUs 0-1**: Llama 3.1 70B Instruct (LLM)\n",
    "- **GPU 2**: NV-CLIP (Visual), NV-EmbedQA (Embeddings), Content Safety\n",
    "- **GPU 3**: Topic Control\n",
    "\n",
    "To change GPU allocation, modify the `device_ids` in `docker-compose-nim-local.yaml`:\n",
    "\n",
    "```yaml\n",
    "deploy:\n",
    "  resources:\n",
    "    reservations:\n",
    "      devices:\n",
    "        - driver: nvidia\n",
    "          device_ids: ['0', '1']  # Change these IDs\n",
    "          capabilities: [gpu]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Optimization\n",
    "For systems with limited RAM:\n",
    "\n",
    "```yaml\n",
    "# Add memory limits to docker-compose.yaml\n",
    "services:\n",
    "  chain-server:\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: 8G\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For production deployments, consider:\n",
    "\n",
    "1. **Load Balancing**: Use a reverse proxy (nginx, traefik) for high availability\n",
    "2. **Monitoring**: Implement Prometheus/Grafana for metrics and alerting\n",
    "3. **Logging**: Centralized logging with ELK stack or similar\n",
    "4. **Security**: Implement proper authentication and authorization\n",
    "5. **Backup**: Regular backups of Milvus and PostgreSQL data\n",
    "6. **Scaling**: Horizontal scaling with Kubernetes for high traffic\n",
    "\n",
    "See the [Deployment Guide](docs/DEPLOYMENT.md) for detailed production setup instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
